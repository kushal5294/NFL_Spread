{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c317436d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 138.8085 - val_loss: 152.6872\n",
      "Epoch 2/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 135.9554 - val_loss: 150.9154\n",
      "Epoch 3/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 133.9291 - val_loss: 149.1434\n",
      "Epoch 4/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 132.0303 - val_loss: 147.3613\n",
      "Epoch 5/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 130.0943 - val_loss: 145.5865\n",
      "Epoch 6/75\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 128.1498 - val_loss: 143.8026\n",
      "Epoch 7/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 126.3673 - val_loss: 142.0573\n",
      "Epoch 8/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 124.2611 - val_loss: 140.3534\n",
      "Epoch 9/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 122.4196 - val_loss: 138.6084\n",
      "Epoch 10/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 120.1026 - val_loss: 136.9223\n",
      "Epoch 11/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 118.0037 - val_loss: 134.9661\n",
      "Epoch 12/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 115.5043 - val_loss: 132.9398\n",
      "Epoch 13/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 113.7164 - val_loss: 130.4637\n",
      "Epoch 14/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 110.9679 - val_loss: 128.5056\n",
      "Epoch 15/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 108.3766 - val_loss: 126.7654\n",
      "Epoch 16/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 106.2440 - val_loss: 125.1437\n",
      "Epoch 17/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 104.2254 - val_loss: 123.6487\n",
      "Epoch 18/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 101.4433 - val_loss: 123.0470\n",
      "Epoch 19/75\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 99.4881 - val_loss: 121.9150\n",
      "Epoch 20/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 97.2025 - val_loss: 121.3000\n",
      "Epoch 21/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 95.1691 - val_loss: 120.9694\n",
      "Epoch 22/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 93.2556 - val_loss: 120.8343\n",
      "Epoch 23/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 91.0966 - val_loss: 120.7027\n",
      "Epoch 24/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 89.4684 - val_loss: 121.0320\n",
      "Epoch 25/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 87.4304 - val_loss: 121.8173\n",
      "Epoch 26/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 85.6153 - val_loss: 122.5517\n",
      "Epoch 27/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 83.7798 - val_loss: 123.4146\n",
      "Epoch 28/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 81.7864 - val_loss: 124.2023\n",
      "Epoch 29/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 80.2809 - val_loss: 125.2135\n",
      "Epoch 30/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 78.2292 - val_loss: 125.9389\n",
      "Epoch 31/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 76.5821 - val_loss: 126.8905\n",
      "Epoch 32/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 74.9691 - val_loss: 127.7536\n",
      "Epoch 33/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 73.2892 - val_loss: 128.5871\n",
      "Epoch 34/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 71.4762 - val_loss: 129.4860\n",
      "Epoch 35/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 69.8316 - val_loss: 130.4600\n",
      "Epoch 36/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 68.3109 - val_loss: 131.6959\n",
      "Epoch 37/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 66.7043 - val_loss: 132.8013\n",
      "Epoch 38/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 65.0510 - val_loss: 133.9639\n",
      "Epoch 39/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 63.4008 - val_loss: 135.1923\n",
      "Epoch 40/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 61.7663 - val_loss: 136.0041\n",
      "Epoch 41/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 60.4652 - val_loss: 137.0695\n",
      "Epoch 42/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 58.8206 - val_loss: 137.8695\n",
      "Epoch 43/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 57.4251 - val_loss: 138.4754\n",
      "Epoch 44/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 55.8265 - val_loss: 138.8999\n",
      "Epoch 45/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 54.3663 - val_loss: 139.4749\n",
      "Epoch 46/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 52.8540 - val_loss: 140.3749\n",
      "Epoch 47/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 51.7193 - val_loss: 141.2546\n",
      "Epoch 48/75\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 50.0520 - val_loss: 142.1243\n",
      "Epoch 49/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 48.5998 - val_loss: 143.0036\n",
      "Epoch 50/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 47.0951 - val_loss: 143.5979\n",
      "Epoch 51/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 45.9178 - val_loss: 144.0559\n",
      "Epoch 52/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 44.6282 - val_loss: 145.2193\n",
      "Epoch 53/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 43.1398 - val_loss: 146.1598\n",
      "Epoch 54/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 42.0575 - val_loss: 147.5469\n",
      "Epoch 55/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 40.7838 - val_loss: 148.7594\n",
      "Epoch 56/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 39.6043 - val_loss: 149.9034\n",
      "Epoch 57/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 38.1916 - val_loss: 150.4213\n",
      "Epoch 58/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 37.2434 - val_loss: 151.5996\n",
      "Epoch 59/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 35.9346 - val_loss: 151.7298\n",
      "Epoch 60/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 34.7045 - val_loss: 152.8384\n",
      "Epoch 61/75\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 33.7971 - val_loss: 153.7967\n",
      "Epoch 62/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 32.6962 - val_loss: 155.4127\n",
      "Epoch 63/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 31.6655 - val_loss: 157.1881\n",
      "Epoch 64/75\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 30.6198 - val_loss: 159.0717\n",
      "Epoch 65/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 29.8636 - val_loss: 159.7688\n",
      "Epoch 66/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 28.8356 - val_loss: 161.5607\n",
      "Epoch 67/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 27.9733 - val_loss: 162.1060\n",
      "Epoch 68/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 27.1529 - val_loss: 163.4686\n",
      "Epoch 69/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 26.3797 - val_loss: 164.5746\n",
      "Epoch 70/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 25.5508 - val_loss: 165.5322\n",
      "Epoch 71/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 24.6163 - val_loss: 165.3368\n",
      "Epoch 72/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 23.9817 - val_loss: 166.0735\n",
      "Epoch 73/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 23.5111 - val_loss: 167.8112\n",
      "Epoch 74/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 22.7174 - val_loss: 169.1261\n",
      "Epoch 75/75\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 21.9288 - val_loss: 169.8014\n",
      "5/5 [==============================] - 0s 584us/step\n",
      "Actual Diff: 8, Predicted Diff: 6.172029495239258\n",
      "Actual Diff: 10, Predicted Diff: 1.915744662284851\n",
      "Actual Diff: -5, Predicted Diff: 14.642328262329102\n",
      "Actual Diff: 3, Predicted Diff: -5.818280220031738\n",
      "Actual Diff: -10, Predicted Diff: -5.833752155303955\n",
      "Actual Diff: 6, Predicted Diff: -3.0517826080322266\n",
      "Actual Diff: -11, Predicted Diff: 21.180131912231445\n",
      "Actual Diff: -10, Predicted Diff: -0.3270433247089386\n",
      "Actual Diff: 3, Predicted Diff: 13.081396102905273\n",
      "Actual Diff: 8, Predicted Diff: 30.741727828979492\n",
      "Actual Diff: -1, Predicted Diff: 2.1319291591644287\n",
      "Actual Diff: 7, Predicted Diff: -2.629948616027832\n",
      "Actual Diff: 7, Predicted Diff: -3.449810266494751\n",
      "Actual Diff: -13, Predicted Diff: 1.3287758827209473\n",
      "Actual Diff: 10, Predicted Diff: 5.581935882568359\n",
      "Actual Diff: -13, Predicted Diff: -7.35547399520874\n",
      "Actual Diff: -6, Predicted Diff: 2.8060433864593506\n",
      "Actual Diff: -37, Predicted Diff: 3.3888254165649414\n",
      "Actual Diff: -7, Predicted Diff: -16.103986740112305\n",
      "Actual Diff: -3, Predicted Diff: -27.00551986694336\n",
      "Actual Diff: -28, Predicted Diff: 3.1829845905303955\n",
      "Actual Diff: -3, Predicted Diff: -11.852155685424805\n",
      "Actual Diff: 8, Predicted Diff: 17.576759338378906\n",
      "Actual Diff: 7, Predicted Diff: 8.756353378295898\n",
      "Actual Diff: 6, Predicted Diff: -6.080820083618164\n",
      "Actual Diff: -4, Predicted Diff: -8.372393608093262\n",
      "Actual Diff: 15, Predicted Diff: 15.13503646850586\n",
      "Actual Diff: 21, Predicted Diff: -11.216172218322754\n",
      "Actual Diff: 6, Predicted Diff: 2.5496864318847656\n",
      "Actual Diff: 13, Predicted Diff: 1.3247219324111938\n",
      "Actual Diff: 1, Predicted Diff: 4.994462013244629\n",
      "Actual Diff: -1, Predicted Diff: -14.242581367492676\n",
      "Actual Diff: -6, Predicted Diff: 4.3351969718933105\n",
      "Actual Diff: 16, Predicted Diff: 13.016907691955566\n",
      "Actual Diff: 13, Predicted Diff: 8.92525863647461\n",
      "Actual Diff: 7, Predicted Diff: 16.725788116455078\n",
      "Actual Diff: -7, Predicted Diff: -8.551111221313477\n",
      "Actual Diff: -14, Predicted Diff: -4.3941826820373535\n",
      "Actual Diff: -3, Predicted Diff: 6.417026042938232\n",
      "Actual Diff: -9, Predicted Diff: 5.121647834777832\n",
      "Actual Diff: 26, Predicted Diff: -4.930318832397461\n",
      "Actual Diff: 5, Predicted Diff: 0.142706960439682\n",
      "Actual Diff: 0, Predicted Diff: 7.391956329345703\n",
      "Actual Diff: 25, Predicted Diff: 15.973783493041992\n",
      "Actual Diff: 1, Predicted Diff: 1.3636785745620728\n",
      "Actual Diff: -13, Predicted Diff: -4.142004489898682\n",
      "Actual Diff: -4, Predicted Diff: 18.35244369506836\n",
      "Actual Diff: 16, Predicted Diff: 0.47787031531333923\n",
      "Actual Diff: 3, Predicted Diff: -13.308542251586914\n",
      "Actual Diff: 7, Predicted Diff: -7.53002405166626\n",
      "Actual Diff: 35, Predicted Diff: 16.7940673828125\n",
      "Actual Diff: 1, Predicted Diff: 7.363434314727783\n",
      "Actual Diff: 1, Predicted Diff: 8.510497093200684\n",
      "Actual Diff: 8, Predicted Diff: 23.757328033447266\n",
      "Actual Diff: 13, Predicted Diff: 9.923460006713867\n",
      "Actual Diff: 4, Predicted Diff: 16.17989730834961\n",
      "Actual Diff: 11, Predicted Diff: -4.846928119659424\n",
      "Actual Diff: -14, Predicted Diff: 1.9674626588821411\n",
      "Actual Diff: -26, Predicted Diff: -7.231524467468262\n",
      "Actual Diff: -2, Predicted Diff: -7.865176200866699\n",
      "Actual Diff: -6, Predicted Diff: -10.913860321044922\n",
      "Actual Diff: 28, Predicted Diff: -3.4357073307037354\n",
      "Actual Diff: -6, Predicted Diff: 6.889350891113281\n",
      "Actual Diff: 6, Predicted Diff: 0.1631668508052826\n",
      "Actual Diff: -14, Predicted Diff: -0.040995389223098755\n",
      "Actual Diff: -8, Predicted Diff: 6.950773239135742\n",
      "Actual Diff: 3, Predicted Diff: 13.985257148742676\n",
      "Actual Diff: 10, Predicted Diff: 3.53592848777771\n",
      "Actual Diff: 3, Predicted Diff: 23.82020378112793\n",
      "Actual Diff: -5, Predicted Diff: -3.759946823120117\n",
      "Actual Diff: 3, Predicted Diff: 1.9932851791381836\n",
      "Actual Diff: -3, Predicted Diff: 7.295987606048584\n",
      "Actual Diff: -8, Predicted Diff: -6.436015605926514\n",
      "Actual Diff: 6, Predicted Diff: 1.1173347234725952\n",
      "Actual Diff: -6, Predicted Diff: 3.368013620376587\n",
      "Actual Diff: 9, Predicted Diff: -8.9650297164917\n",
      "Actual Diff: 6, Predicted Diff: 7.433184623718262\n",
      "Actual Diff: 3, Predicted Diff: -0.2074386179447174\n",
      "Actual Diff: -11, Predicted Diff: -21.061670303344727\n",
      "Actual Diff: -8, Predicted Diff: 1.6821361780166626\n",
      "Actual Diff: 12, Predicted Diff: -0.9737234115600586\n",
      "Actual Diff: -16, Predicted Diff: 3.5022099018096924\n",
      "Actual Diff: -22, Predicted Diff: -2.1871795654296875\n",
      "Actual Diff: -7, Predicted Diff: -2.4101803302764893\n",
      "Actual Diff: 14, Predicted Diff: 5.850063323974609\n",
      "Actual Diff: 3, Predicted Diff: 8.393502235412598\n",
      "Actual Diff: -4, Predicted Diff: -8.603802680969238\n",
      "Actual Diff: 14, Predicted Diff: 2.228963613510132\n",
      "Actual Diff: 8, Predicted Diff: 15.196416854858398\n",
      "Actual Diff: -5, Predicted Diff: 2.9574811458587646\n",
      "Actual Diff: 17, Predicted Diff: 16.6268367767334\n",
      "Actual Diff: 6, Predicted Diff: 3.457061529159546\n",
      "Actual Diff: 3, Predicted Diff: -4.250957489013672\n",
      "Actual Diff: -6, Predicted Diff: 7.678755283355713\n",
      "Actual Diff: 37, Predicted Diff: 7.045327186584473\n",
      "Actual Diff: -3, Predicted Diff: -7.323469161987305\n",
      "Actual Diff: -17, Predicted Diff: -5.911378860473633\n",
      "Actual Diff: -14, Predicted Diff: -18.35369873046875\n",
      "Actual Diff: 1, Predicted Diff: -0.07239393889904022\n",
      "Actual Diff: 31, Predicted Diff: 17.30487823486328\n",
      "Actual Diff: 3, Predicted Diff: 12.53884506225586\n",
      "Actual Diff: 2, Predicted Diff: 16.556737899780273\n",
      "Actual Diff: 28, Predicted Diff: 14.46053695678711\n",
      "Actual Diff: -10, Predicted Diff: 12.937188148498535\n",
      "Actual Diff: 6, Predicted Diff: 5.473130226135254\n",
      "Actual Diff: -14, Predicted Diff: -6.36223030090332\n",
      "Actual Diff: -28, Predicted Diff: 7.813102722167969\n",
      "Actual Diff: -3, Predicted Diff: 5.570013046264648\n",
      "Actual Diff: 17, Predicted Diff: -3.4957032203674316\n",
      "Actual Diff: 24, Predicted Diff: -4.9469451904296875\n",
      "Actual Diff: 21, Predicted Diff: -6.784106731414795\n",
      "Actual Diff: -3, Predicted Diff: 9.7608003616333\n",
      "Actual Diff: 0, Predicted Diff: -2.187795400619507\n",
      "Actual Diff: -18, Predicted Diff: -16.678939819335938\n",
      "Actual Diff: 4, Predicted Diff: 1.012803554534912\n",
      "Actual Diff: 13, Predicted Diff: 2.4821949005126953\n",
      "Actual Diff: 12, Predicted Diff: 28.62320327758789\n",
      "Actual Diff: -16, Predicted Diff: 3.0998306274414062\n",
      "Actual Diff: 11, Predicted Diff: 10.958681106567383\n",
      "Actual Diff: -1, Predicted Diff: -7.268034934997559\n",
      "Actual Diff: 5, Predicted Diff: 4.92263126373291\n",
      "Actual Diff: -3, Predicted Diff: 5.315673828125\n",
      "Actual Diff: 14, Predicted Diff: -7.389020919799805\n",
      "Actual Diff: 3, Predicted Diff: -9.356818199157715\n",
      "Actual Diff: 6, Predicted Diff: 18.456512451171875\n",
      "Actual Diff: 25, Predicted Diff: 7.2671661376953125\n",
      "Actual Diff: 3, Predicted Diff: 0.9730691313743591\n",
      "Actual Diff: 20, Predicted Diff: -15.67660140991211\n",
      "Actual Diff: -4, Predicted Diff: 9.23167610168457\n",
      "Actual Diff: 18, Predicted Diff: 9.792442321777344\n",
      "Actual Diff: 1, Predicted Diff: -8.837457656860352\n",
      "Actual Diff: 3, Predicted Diff: 23.82020378112793\n",
      "Actual Diff: -7, Predicted Diff: 8.393502235412598\n",
      "Actual Diff: 7, Predicted Diff: 10.958681106567383\n",
      "Actual Diff: -17, Predicted Diff: -3.076711893081665\n",
      "Actual Diff: 7, Predicted Diff: 5.081892013549805\n",
      "Actual Diff: 31, Predicted Diff: 18.456512451171875\n",
      "Actual Diff: -17, Predicted Diff: 19.876880645751953\n",
      "Actual Diff: 7, Predicted Diff: 3.56894850730896\n",
      "Actual Diff: 24, Predicted Diff: 14.21107292175293\n",
      "Actual Diff: 3, Predicted Diff: 1.048810601234436\n",
      "Actual Diff: -3, Predicted Diff: -10.230327606201172\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load data from CSV files\n",
    "scores_df = pd.read_csv(\"2022_scores.csv\")\n",
    "sample_data_df = pd.read_csv(\"2022_sample_data.csv\")\n",
    "\n",
    "# Merge the data based on \"Away Team\" and \"Home Team\"\n",
    "merged_df = pd.merge(scores_df, sample_data_df, left_on=\"Away Team\", right_on=\"Tm\", how=\"left\")\n",
    "merged_df = pd.merge(merged_df, sample_data_df, left_on=\"Home Team\", right_on=\"Tm\", suffixes=(\"_away\", \"_home\"), how=\"left\")\n",
    "\n",
    "# Define features and target\n",
    "features = [\n",
    "    \"OffRk_away\", \"G_away\", \"PF_away\", \"Tot_Yds_away\", \"Ply_away\", \"Y/P_away\", \"TO_away\", \"FL_away\",\n",
    "    \"Tot_1stD_away\", \"Cmp_away\", \"P_Att_away\", \"P_Yds_away\", \"P_TD_away\", \"Int_away\", \"NY/A_away\", \"P_1stD_away\",\n",
    "    \"R_Att_away\", \"R_Yds_away\", \"R_TD_away\", \"Y/A_away\", \"R_1stD_away\", \"Pen_away\", \"Pen_Yds_away\", \"1stPy_away\",\n",
    "    \"Sc%_away\", \"TO%_away\", \"EXP_away\",\n",
    "    \"OffRk_home\", \"G_home\", \"PF_home\", \"Tot_Yds_home\", \"Ply_home\", \"Y/P_home\", \"TO_home\", \"FL_home\",\n",
    "    \"Tot_1stD_home\", \"Cmp_home\", \"P_Att_home\", \"P_Yds_home\", \"P_TD_home\", \"Int_home\", \"NY/A_home\", \"P_1stD_home\",\n",
    "    \"R_Att_home\", \"R_Yds_home\", \"R_TD_home\", \"Y/A_home\", \"R_1stD_home\", \"Pen_home\", \"Pen_Yds_home\", \"1stPy_home\",\n",
    "    \"Sc%_home\", \"TO%_home\", \"EXP_home\"\n",
    "]\n",
    "target = \"Diff\"\n",
    "\n",
    "# Split the data into training and testing\n",
    "split_point = len(merged_df) // 2\n",
    "train_data = merged_df.iloc[:split_point]\n",
    "test_data = merged_df.iloc[split_point:]\n",
    "\n",
    "X_train = train_data[features]\n",
    "y_train = train_data[target]\n",
    "X_test = test_data[features]\n",
    "y_test = test_data[target]\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Build the regression model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(units=64, activation=\"relu\", input_shape=(X_train.shape[1],)),\n",
    "    keras.layers.Dense(units=32, activation=\"relu\"),\n",
    "    keras.layers.Dense(units=1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=75, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Predict \"Diff\" values for the test data\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Print the predicted \"Diff\" values\n",
    "for i, prediction in enumerate(predictions):\n",
    "    print(f\"Actual Diff: {y_test.iloc[i]}, Predicted Diff: {prediction[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a66f72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
